from flask import Flask, request, jsonify, Response, copy_current_request_context
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer  # æ–°å¢ TextStreamer

# æ¨¡å‹è·¯å¾„å’Œè®¾å¤‡é…ç½®ï¼ˆä¿æŒä¸å˜ï¼‰
MODEL_PATH = "D:/NeuroSync/NeuroSync_Player-main/utils/llm/local_api/deepseek_7b_chat/ckpt"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Activated device: {device}")

app = Flask(__name__)

# æ„å»ºDeepSeekæ ¼å¼çš„Promptï¼ˆä¿æŒä¸å˜ï¼‰
def build_deepseek_prompt(messages):
    prompt = "<s>"
    for msg in messages:
        role = msg["role"]
        content = msg["content"].strip()
        prompt += f"{role}: {content}\n</s>"
    prompt += "assistant: "
    return prompt

# ç”Ÿæˆé…ç½®ï¼ˆä¿æŒä¸å˜ï¼‰
generation_config = GenerationConfig(
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1,
    do_sample=True,
)

# åŠ è½½æ¨¡å‹å’Œtokenizerï¼ˆä¿æŒä¸å˜ï¼‰
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    dtype=torch.float16 if device == "cuda" else torch.float32,
    trust_remote_code=True
)
model.eval()
print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# æ–°å¢ï¼šæµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸ç”Ÿæˆå†…å®¹
print("\n===== æµ‹è¯•æ¨¡å‹ç”Ÿæˆèƒ½åŠ› =====")
test_prompt = "<s>user: ä½ å¥½ï¼Œèƒ½çœ‹åˆ°è¿™å¥è¯å—ï¼Ÿ</s>assistant: "
test_inputs = tokenizer(test_prompt, return_tensors="pt").to(device)
try:
    test_outputs = model.generate(** test_inputs, max_new_tokens=20)
    test_response = tokenizer.decode(test_outputs[0], skip_special_tokens=True)
    print(f"æ¨¡å‹æµ‹è¯•è¾“å‡ºï¼š{test_response}")
except Exception as e:
    print(f"æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼š{str(e)}")
print("===========================\n")

# éæµå¼æ¥å£ï¼ˆä¿æŒä¸å˜ï¼‰
@app.route("/generate_llama", methods=["POST"])
def generate_llama():
    if not request.is_json:
        return jsonify({"error": "è¯·æ±‚æ ¼å¼å¿…é¡»ä¸ºJSON"}), 400
    request_data = request.get_json()
    messages = request_data.get("messages", [])
    if not messages:
        return jsonify({"error": "ç¼ºå°‘messageså‚æ•°"}), 400
    
    prompt = build_deepseek_prompt(messages)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)
    outputs = model.generate(** inputs, generation_config=generation_config)
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True).split("assistant: ")[-1]
    return jsonify({"response": full_response})

# ä¿®æ­£åçš„æµå¼æ¥å£ï¼šé¢„æå–å‚æ•°ï¼Œé¿å…ç”Ÿæˆå™¨ä¾èµ–request
@app.route("/generate_stream", methods=["POST"])
def generate_stream():
    # ç¬¬ä¸€æ­¥ï¼šé¢„æå–å‚æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
    if not request.is_json:
        return Response(
            f"data: {json.dumps({'error': 'è¯·æ±‚æ ¼å¼å¿…é¡»ä¸ºJSON'})}\n\n",
            mimetype="text/event-stream"
        )
    request_data = request.get_json()
    messages = request_data.get("messages", [])
    if not messages:
        return Response(
            f"data: {json.dumps({'error': 'ç¼ºå°‘messageså‚æ•°'})}\n\n",
            mimetype="text/event-stream"
        )
    
    # æ„å»ºPromptï¼ˆä¿æŒä¸å˜ï¼‰
    prompt = build_deepseek_prompt(messages)
   # ğŸ‘‡ æ–°å¢æ—¥å¿—æ”¾åœ¨è¿™é‡Œ
    print(f"===== ç”Ÿæˆçš„Promptå†…å®¹ =====")
    print(prompt)
    print("===========================")
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)

    # ç¬¬äºŒæ­¥ï¼šå®šä¹‰ç”Ÿæˆå™¨ï¼Œä½¿ç”¨TextStreameræ›¿ä»£as_streamer
    @copy_current_request_context
    def generate():
        try:
            print("\n=== æµå¼è¯·æ±‚å·²æ¥æ”¶ ===")
            print("=== å¼€å§‹æµå¼ç”Ÿæˆ ===")
        
            # ç”Ÿæˆå®Œæ•´å“åº”
            outputs = model.generate(
                **inputs,
                generation_config=generation_config,
                output_scores=False
            )
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            assistant_response = full_response.split("assistant: ")[-1].strip()
            
            # æŒ‰è¯è¯­/æ ‡ç‚¹åˆ†å‰²ï¼ˆä¿æŒè‡ªç„¶æ–­å¥ï¼‰
            import re
            chunks = re.split(r'([ï¼Œã€‚, .!?ï¼›;])', assistant_response)
            chunks = [c for c in chunks if c.strip()]
            merged_chunks = []
            for i in range(len(chunks)):
                if i % 2 == 1:
                    if merged_chunks:
                        merged_chunks[-1] += chunks[i]
                    else:
                        merged_chunks.append(chunks[i])
                else:
                    merged_chunks.append(chunks[i])
            
            # ç›´æ¥è¿”å›çº¯æ–‡æœ¬ï¼Œä¸æ·»åŠ ä»»ä½•å‰ç¼€
            for chunk in merged_chunks:
                if chunk:
                    print(f"ç”Ÿæˆåˆ†å—ï¼š{chunk}")
                    # ä»…è¿”å›æ–‡æœ¬å†…å®¹ï¼ŒåŠ æ¢è¡Œç¬¦åˆ†éš”
                    yield f"{chunk}\n"
            
            print("=== æµå¼ç”Ÿæˆç»“æŸ ===")
        
        except Exception as e:
            error_msg = f"é”™è¯¯ï¼š{str(e)}"
            print(error_msg)
            yield f"{error_msg}\n"  # é”™è¯¯ä¿¡æ¯ä¹Ÿä»…è¿”å›çº¯æ–‡æœ¬

    # å“åº”ç±»å‹è®¾ä¸ºçº¯æ–‡æœ¬
    return Response(generate(), mimetype="text/plain; charset=utf-8")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5050, threaded=False, debug=False)
